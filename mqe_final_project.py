# -*- coding: utf-8 -*-
"""MQE 425 Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/113sA_sWxJkaC7DBNFk4VyLrly6e6XK_d

Econ 425 Final Project  
Nathan Kim(805182816), Kamakshi Sirpal(305864760), Daniel Wang(105867284)

Code for Importing and Cleaning Data (Daniel Wang)
"""

from google.colab import drive
drive.mount('/content/drive')

import os
path = "/content/drive/My Drive/425 Project" # Your path here
os.chdir(path)

import numpy as np 
import pandas as pd
import seaborn as sns 
import matplotlib.pyplot as plt

#importing subset of meps file
import pandas as pd
meps_sub=pd.read_csv('meps_sub.csv')
meps_sub.drop(['Unnamed: 0'], axis=1, inplace=True)

# code to import conditions file to get ICD codes
import pandas as pd
conditions=pd.read_excel('h214.xlsx')

# subsetting conditions file 
cond=conditions[['DUPERSID', 'ICD10CDX']]

# condensing ICD columns to just letter code
cond['ICD_CAT']=cond['ICD10CDX'].str[:1]

cond

# creating high level dataset where ICD_CAT is a single column for visualizations
high=cond.merge(meps_sub, on='DUPERSID')

# output high level dataset
high.to_csv('high.csv')

# calculating number of unique ICD codes
high['ICD10CDX'].nunique()

# turning ICD codes to dummy variables
dumb = pd.get_dummies(cond['ICD_CAT'],drop_first=False)

# merging dummies back to DUPERSID
cond_dumb=pd.concat([cond, dumb], axis=1)

# dropping original ICD columns before merging by DUPERSID
cond_dumb.drop(["ICD10CDX", "ICD_CAT"], axis=1, inplace=True)

# merging rows by DUPERSID
cond_max=cond_dumb.groupby('DUPERSID').max().reset_index()

# merge to demographic columns
full=cond_max.merge(meps_sub, on='DUPERSID')

# dropping rows with ICD = -15
final=full.drop(full.index[full['-']==1])

#dropping - column
final.drop(["-"], axis=1, inplace=True)

len(full)

len(final)

# SECTION TO TURN POVERTY CATEGORIES INTO BINARY AT OR BELOW(1) AND ABOVE(0)

#importing file with clean ICD data
import pandas as pd
final=pd.read_csv('final.csv')
final.drop(['Unnamed: 0'], axis=1, inplace=True)

final['POV']=final['POVCAT19']==1

final['POV']=final['POV']*1

final.to_csv('final.csv')

"""Code for Data Exploration and Visualization (Daniel Wang)"""

from google.colab import drive
drive.mount('/content/drive')

import os
path = "/content/drive/My Drive/425 Project" # Your path here
os.chdir(path)

#importing file with clean ICD data (cleaned from section above)
import pandas as pd
final=pd.read_csv('final.csv')
final.drop(['Unnamed: 0'], axis=1, inplace=True)

#importing high level data with all ICD values in a single column, may be helpful for visualization
import pandas as pd
high=pd.read_csv('high.csv')
high.drop(['Unnamed: 0'], axis=1, inplace=True)

# dropping missing rows from high level data
high=high.drop(high.index[high['ICD_CAT']=='-'])

sns.histplot(data=final, x='POVLEV19')

"""Linear Regression, Logistic Regression, and SVM (Kamakshi Sirpal)

## Linear Regression
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from sklearn.linear_model import LinearRegression #sklearn.linear_model contains all LinearRegression, Lasso, Ridge, ElasticNet
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

import os
path = "/content/drive/My Drive/425 Project" # Your path here
os.chdir(path)

#importing file with clean ICD data
import pandas as pd
df=pd.read_csv('final.csv')
df.drop(['Unnamed: 0'], axis=1, inplace=True)

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
# Load the dataset
df

# General info of all the columns in our dataset
df.info()

#Converting X and Y to arrays
final_X=np.array(df.iloc[:,2:21])
final_Y=np.array(df['POVLEV19'])

final_X

final_Y

final_Y.shape

# Linear regression model
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(final_X, final_Y, test_size=0.30, random_state=0)

lr = LinearRegression().fit(X_train, y_train)

print("lr.coef_:", lr.coef_)

#Checking the training and testing set scores
print("Training set score: {:.2f}".format(lr.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lr.score(X_test, y_test)))

#Using the model to predict on X test data
y_pred = lr.predict(X_test)
y_pred

y_pred.shape

#Exploring various metrics to evaluate model performance
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print("MSE:", mse)

from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)
print("MAE:", mae)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE:", rmse)

# importing gradient descent function
import math
import numpy as np
import pandas as pd
from pandas import DataFrame
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
#from sklearn.cross_validation import train_test_split
from sklearn.model_selection import train_test_split
from numpy import loadtxt, where
from pylab import scatter, show, legend, xlabel, ylabel

##implementation of sigmoid function
def Sigmoid(x):
	g = float(1.0 / float((1.0 + math.exp(-1.0*x))))
	return g

##Prediction function
def Prediction(theta, x):
	z = 0
	for i in range(len(theta)):
		z += x[i]*theta[i]
	return Sigmoid(z)


# implementation of cost functions
def Cost_Function(X,Y,theta,m):
	sumOfErrors = 0
	for i in range(m):
		xi = X[i]
		est_yi = Prediction(theta,xi)
		if Y[i] == 1:
			error = Y[i] * math.log(est_yi)
		elif Y[i] == 0:
			error = (1-Y[i]) * math.log(1-est_yi)
		sumOfErrors += error
	const = -1/m
	J = const * sumOfErrors
	#print 'cost is ', J 
	return J

 
# gradient components called by Gradient_Descent()

def Cost_Function_Derivative(X,Y,theta,j,m,alpha):
	sumErrors = 0
	for i in range(m):
		xi = X[i]
		xij = xi[j]
		hi = Prediction(theta,X[i])
		error = (hi - Y[i])*xij
		sumErrors += error
	m = len(Y)
	constant = float(alpha)/float(m)
	J = constant * sumErrors
	return J

# execute gradient updates over thetas
def gradientDescent(X,Y,theta,m,alpha):
	new_theta = []
	constant = alpha/m
	for j in range(len(theta)):
		deltaF = Cost_Function_Derivative(X,Y,theta,j,m,alpha)
		new_theta_value = theta[j] - deltaF
		new_theta.append(new_theta_value)
	return new_theta

# call the GD algorithm, placeholders in the function gradientDescent()
theta = lr.coef_
ALPHA = 1
MAX_ITER = 500
[theta, arrCost] = gradientDescent(X_train, y_train, theta, ALPHA, MAX_ITER)

#visualize the convergence curve
plt.plot(range(0,len(arrCost)),arrCost);
plt.xlabel('itr')
plt.ylabel('cost val')
plt.title('alpha = {}'.format(ALPHA))
plt.show()

# call the GD algorithm, placeholders in the function gradientDescent()
theta = lr.coef_
ALPHA = 0.1
MAX_ITER = 500
[theta, arrCost] = gradientDescent(X_train, y_train, theta, ALPHA, MAX_ITER)

#visualize the convergence curve
plt.plot(range(0,len(arrCost)),arrCost);
plt.xlabel('itr')
plt.ylabel('cost val')
plt.title('alpha = {}'.format(ALPHA))
plt.show()

# call the GD algorithm, placeholders in the function gradientDescent()
theta = lr.coef_
ALPHA = 0.1
MAX_ITER = 600
[theta, arrCost] = gradientDescent(X_train, y_train, theta, ALPHA, MAX_ITER)

#visualize the convergence curve
plt.plot(range(0,len(arrCost)),arrCost);
plt.xlabel('itr')
plt.ylabel('cost val')
plt.title('alpha = {}'.format(ALPHA))
plt.show()

final_Y.shape

final_X.shape

print(y_test)

print(y_pred)

X_test.shape

#Checking the parameters for linear regression
para = LinearRegression().get_params(deep=True)
para

"""## Logistic Regression"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from sklearn.linear_model import LogisticRegression #sklearn.linear_model contains all LinearRegression, Lasso, Ridge, ElasticNet
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns

# defining gradient descent logistic function
import math
import numpy as np
import pandas as pd
from pandas import DataFrame
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
#from sklearn.cross_validation import train_test_split
from sklearn.model_selection import train_test_split
from numpy import loadtxt, where
from pylab import scatter, show, legend, xlabel, ylabel

##implementation of sigmoid function
def Sigmoid(x):
	g = float(1.0 / float((1.0 + math.exp(-1.0*x))))
	return g

##Prediction function
def Prediction(theta, x):
	z = 0
	for i in range(len(theta)):
		z += x[i]*theta[i]
	return Sigmoid(z)


# implementation of cost functions
def Cost_Function(X,Y,theta,m):
	sumOfErrors = 0
	for i in range(m):
		xi = X[i]
		est_yi = Prediction(theta,xi)
		if Y[i] == 1:
			error = Y[i] * math.log(est_yi)
		elif Y[i] == 0:
			error = (1-Y[i]) * math.log(1-est_yi)
		sumOfErrors += error
	const = -1/m
	J = const * sumOfErrors
	#print 'cost is ', J 
	return J

 
# gradient components called by Gradient_Descent()

def Cost_Function_Derivative(X,Y,theta,j,m,alpha):
	sumErrors = 0
	for i in range(m):
		xi = X[i]
		xij = xi[j]
		hi = Prediction(theta,X[i])
		error = (hi - Y[i])*xij
		sumErrors += error
	m = len(Y)
	constant = float(alpha)/float(m)
	J = constant * sumErrors
	return J

# execute gradient updates over thetas
def (X,Y,theta,m,alpha):
	new_theta = []
	constant = alpha/m
	for j in range(len(theta)):
		deltaF = Cost_Function_Derivative(X,Y,theta,j,m,alpha)
		new_theta_value = theta[j] - deltaF
		new_theta.append(new_theta_value)
	return new_theta

## Understanding Data
# Explore the balance of counts for target variable
df['POV'].value_counts()

#Checking for unbalanced data
sns.countplot(x='POV', data=df)

## Poverty distribution by each category
# A
pd.crosstab(df.A,df.POV).plot(kind='bar')
plt.title('Poverty Level by category A')
plt.xlabel('A')
plt.ylabel('Frequency of Poverty')

#Numerical data of above graph
pd.crosstab(df.A,df.POV)

# B
pd.crosstab(df.B,df.POV).plot(kind='bar')
plt.title('Frequency of Poverty by category B')
plt.xlabel('B')
plt.ylabel('Frequency of Poverty')

#Trying to see the proportion and not absolute numbers for category B
table.div(table.sum(1).astype(float), axis=0)

df_final_X = df[['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'Z']]

df_final_Y = df[['POV']]

#Splitting the data into training and testing sets with test size of 30%
X_train,X_test,Y_train,Y_test = train_test_split(df_final_X,df_final_Y,test_size=0.30)

# Run logistic regression
logistic = LogisticRegression() 
logistic.fit(X_train,Y_train) 
print("Test set score: {:.2f}".format(logistic.score(X_test, Y_test))) # Accuracy Score

#10 fold cross validation of accuracy scores
from sklearn.model_selection import cross_val_score

scores = cross_val_score(logistic, X_train, Y_train, cv=10)
print('Cross-Validation Accuracy Scores', scores)

#Looking at just the minimum, maximum and mean accuracy scores
scores = pd.Series(scores)
scores.min(), scores.mean(), scores.max()

#Updating the solver for logistic regression (tuning the hyperparameter)
logistic1 = LogisticRegression(solver='liblinear')
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

logistic1 = LogisticRegression(solver='newton-cg')
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

logistic1 = LogisticRegression(solver='lbfgs')
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

logistic1 = LogisticRegression(solver='sag')
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

logistic1 = LogisticRegression(solver='saga')
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

#Adjusting the type of regularisation applied
logistic1 = LogisticRegression(penalty='none')
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

#Ensuring the right solver is paired with the regularisation term
logistic1 = LogisticRegression(solver='liblinear', penalty='l1')
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

#Adjusting the penalty strength C
logistic1 = LogisticRegression(C=1.0)
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

logistic1 = LogisticRegression(C=100)
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

logistic1 = LogisticRegression(C=10)
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

logistic1 = LogisticRegression(C=0.1)
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

logistic1 = LogisticRegression(C=0.01)
logistic1.fit(X_train,Y_train)
print("Test set score: {:.5f}".format(logistic1.score(X_test, Y_test))) # Accuracy Score

#Finding the y score using the decision function in sklearn
from sklearn.multiclass import OneVsRestClassifier
from sklearn import svm, datasets
from sklearn.metrics import roc_curve
# Learn to predict each class against the other
random_state = np.random.RandomState(0)
classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,
                                 random_state=random_state))
y_score = classifier.fit(X_train, Y_train).decision_function(X_test)

roc_curve(Y_test, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)

#Finding the false positive and true positive rates to plot ROC curve
fpr, tpr, _ = metrics.roc_curve(Y_test,  y_score)

#create ROC curve
plt.plot(fpr,tpr)
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

y_test.shape

param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}
param['nthread'] = 4
param['eval_metric'] = 'auc'

"""## SVM Model"""

#Import svm model
from sklearn import svm

#Create a svm Classifier
clf = svm.SVC(kernel='linear') # Linear Kernel

#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

# Model Precision: what percentage of positive tuples are labeled as such?
print("Precision:",metrics.precision_score(Y_test, y_pred))

# Model Recall: what percentage of positive tuples are labelled as such?
print("Recall:",metrics.recall_score(Y_test, y_pred))

#Adjusting the kernel
clf1 = svm.SVC(kernel='rbf')
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(kernel='linear')
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(kernel='poly')
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(kernel='sigmoid')
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(kernel='precomputed')
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

#Changing the penalty strength C to see if it affects model performance
clf1 = svm.SVC(C=0.1)
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(C=10)
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(C=0.01)
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(C=100)
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

#Adjusting the hyperparameter gamma (kernel coefficient) for optimisation
clf1 = svm.SVC(gamma='auto')
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(gamma='1')
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

clf1 = svm.SVC(gamma='-100')
#Train the model using the training sets
clf.fit(X_train, Y_train.values.ravel())

#Predict the response for test dataset
y_pred = clf.predict(X_test)
# Model Accuracy: how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))

"""Neural Network Analysis (Nathan Kim)"""

# Import modules 

import random 
import itertools 

import numpy as np 
import pandas as pd
import seaborn as sns 
import matplotlib.pyplot as plt 

from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras import Sequential, layers
from tensorflow.keras.regularizers import l1
from tensorflow.keras.metrics import Accuracy
from tensorflow.keras.losses import BinaryCrossentropy

"""Set up data"""

from google.colab import drive
drive.mount('/content/drive')

import os
path = "/content/drive/My Drive/425 Project" # Your path here
os.chdir(path)

#importing file with clean ICD data
import pandas as pd
data=pd.read_csv('final.csv')
data.drop(['Unnamed: 0'], axis=1, inplace=True)

# Features : ICD Codes (binary)
X = data[['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',
          'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'Z']]

# Target : Poverty status (binary)
y = data['POV']

# Convert to np arrays 
X = np.array(X)
y = np.array(y)

# Train/test split 
# We will only use X_train and y_train for training 
# X_test and y_test will only be used at the end for testing 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

"""Design preliminary NN architecture (no cross validation)"""

# Set hyperparameters 
opt_adam1 = tf.keras.optimizers.Adam(learning_rate=0.0001)
opt_adam2 = tf.keras.optimizers.Adam(learning_rate=0.001)

m1 = Sequential([
    layers.Dense(12, activation='relu', input_shape=(20,)), # 12 neurons in the 1st hidden layer 
    layers.Dense(1,  activation='sigmoid')                  # Binary outcome 
])

m2 = Sequential([
    layers.Dense(12, activation='relu', input_shape=(20,)), # 12 neurons in 1st hidden layer 
    layers.Dense(9,  activation='relu'),                    #  9 neurons in 2nd hidden layer 
    layers.Dense(1,  activation='sigmoid')                  #  Binary outcome 
])

m3 = Sequential([
    layers.Dense(12, activation='relu', input_shape=(20,)), # 12 neurons in the 1st hidden layer
    layers.Dense(9,  activation='relu'),                    #  9 neurons in the 2nd hidden layer 
    layers.Dense(6,  activation='relu'),                    #  6 neurons in the 3rd hidden layer 
    layers.Dense(1,  activation='sigmoid')                  #  Binary outcome
])

m4 = Sequential([
    layers.Dense(12, activation='relu', input_shape=(20,)), # 12 neurons in the 1st hidden layer  
    layers.Dense(9,  activation='relu'),                    #  9 neurons in the 2nd hidden layer 
    layers.Dense(6,  activation='relu'),                    #  6 neurons in the 3rd hidden layer
    layers.Dense(3,  activation='relu'),                    #  3 neurons in the 4th hidden layer
    layers.Dense(1,  activation='sigmoid')                  #  Binary outcome 
])

"""Compile preliminary models """

# Compiles with the Adam optimizer specified above 
# Uses the cross entropy loss function and accuracy as metric 

m1.compile(optimizer=opt_adam1, loss=BinaryCrossentropy(),
           metrics=['accuracy'])

m2.compile(optimizer=opt_adam1, loss=BinaryCrossentropy(), 
           metrics=['accuracy'])

m3.compile(optimizer=opt_adam1, loss=BinaryCrossentropy(), 
           metrics=['accuracy'])

m4.compile(optimizer=opt_adam1, loss=BinaryCrossentropy(), 
           metrics=['accuracy'])

"""Define function to plot loss convergence"""

# Plots loss convergence from training process 

def plot_loss(history, title):
    plt.figure(figsize=(6, 4))
    plt.title('Loss Convergence for ' + title)
    plt.plot(history.history['loss'], label='Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.legend()
    plt.grid(True)

"""Fit models and obtain preliminary results"""

fit1 = m1.fit(X_train, y_train, epochs=30, 
              verbose=0, validation_split=0.3)

plot_loss(fit1, 'Model 1 : [12]')

m1_loss, m1_acc = m1.evaluate(X_test, y_test, verbose=2)

fit2 = m2.fit(X_train, y_train, epochs=30, 
              verbose=0, validation_split=0.3)

plot_loss(fit2, 'Model 2 : [12, 9]')

m2_loss, m2_acc = m2.evaluate(X_test, y_test, verbose=2)

fit3 = m3.fit(X_train, y_train, epochs=30, 
              verbose=0, validation_split=0.3)

plot_loss(fit3, 'Model 3 : [12, 9, 6]')

m3_loss, m3_acc = m3.evaluate(X_test, y_test, verbose=2)

fit4 = m4.fit(X_train, y_train, epochs=30, 
              verbose=0, validation_split=0.3)

plot_loss(fit4, 'Model 4 : [12, 9, 6, 3]')

m4_loss, m4_acc = m4.evaluate(X_test, y_test, verbose=2)

"""Hyperparemeter tuning (Cross validation for grid search)"""

# Candidate learning rates 
adam_alpha = np.linspace(0.0001, 0.001, 10)

# Candidate L1 coefficients 
regl_coeff = np.linspace(0.0001, 0.05, 10)

# mspec   : number of neurons in each layer from 1st hidden layer to last hidden layer 
# ex) [12, 9, 6] -> 12 in 1st hidden layer, 9 in 2nd hidden layer, etc. 

# adam    : list of candidate learning rates for Adam optimizer
# regl    : list of candidate L1 coefficients 
# returns : compiled feed forward neural network model 

def build_model(mspec, adam, regl):
  
    tf.random.set_seed(123)
    np.random.seed(123)
    random.seed(123)
    
    model = Sequential() # Feed forward neural network 
    
    for m in mspec: # For each layer in model specification 
        
        if regl == 0: # If candidate L1 coefficients not specified/included
            model.add(layers.Dense(m, activation='relu')) # Layer with no regularizer 
            
        else: # If candidate L1 coefficients are specified/included 
            model.add(layers.Dense(m, activation='relu', kernel_regularizer=l1(regl))) # Layer with L1 norm 
            
    model.add(layers.Dense(1, activation='sigmoid')) # Final output layer
    
    opt_adam = tf.keras.optimizers.Adam(learning_rate=adam) # Adam optimizer with specified learning rate 
    
    model.compile(optimizer=opt_adam, loss=BinaryCrossentropy(), 
                  metrics=['accuracy']) # Compile model 
                  
    return model

# mspec   : model specification 
# adam    : list of candidate learning rates for Adam optimizer
# X, y    : training data as numpy arrays 
# returns : mean accuracy across folds for each learning rate 

def grid_search(mspec, adam, X, y): 
  
    tf.random.set_seed(123)
    np.random.seed(123)
    random.seed(123)
    
    adam_values = [] # Stores current learning rate
    accuracies  = [] # Stores current mean accuracy 
    
    kf = KFold(n_splits=5, random_state=0, shuffle=True) # 5-fold split 
    
    for a in adam: # For each candidate learning rate
        
        model = build_model(mspec, a, 0) # Build model with specified learning rate 
        fold_acc = [] # Stores accuracies from each fold 
        
        for train_idx, test_idx in kf.split(X, y): # Fit model for each fold 
            model.fit(X[train_idx], y[train_idx],
                      epochs=5, batch_size=512, verbose=0)
            
            loss, acc = model.evaluate(X[test_idx], y[test_idx], verbose=0) # Accuracy from each fold 
            fold_acc.append(acc) # Accuracy from each fold  
            
        mean_acc = np.mean(fold_acc) # Get mean accuracy from all 5 folds 
        
        adam_values.append(a) # Append current learning rate 
        accuracies.append(mean_acc) # Append current mean accuracy 
            
    return pd.DataFrame({'Learning Rate' : adam_values,
                         'Accuracy'      : accuracies})

# mspec   : model specification
# adam    : list of candidate learning rates for Adam optimizer
# regl    : list of candidate L1 coefficients 
# X, y    : training data as numpy arrays 
# returns : mean accuracy across folds for each combination of 
#           learning rates and L1 coefficients 

def grid_search_regl(mspec, adam, regl, X, y):

    tf.random.set_seed(123)
    np.random.seed(123)
    random.seed(123)
    
    adam_values = [] # Stores current learning rate
    regl_values = [] # Stores current L1 coefficient
    accuracies  = [] # Stores current mean accuracy
    
    kf = KFold(n_splits=5, random_state=0, shuffle=True) # 5-fold split 
    
    grid = list(itertools.product(adam, regl)) # All combinations of L1 coefficients and learning rates
    
    for g in grid: # Same process as grid_search() function above 
        
        model = build_model(mspec, g[0], g[1]) # Build/compile model with specified hyperparameters
        fold_acc = [] 
        
        for train_idx, test_idx in kf.split(X, y): # Fit model for each fold 
            model.fit(X[train_idx], y[train_idx],
                      epochs=5, batch_size=512, verbose=0)
            
            loss, acc = model.evaluate(X[test_idx], y[test_idx], verbose=0) # Accuracy from each fold 
            fold_acc.append(acc) # Append ccuracy from each fold 
            
        mean_acc = np.mean(fold_acc) # Get mean accuracy from all 5 folds 
        
        adam_values.append(g[0]) # Append current learning rate 
        regl_values.append(g[1]) # Append current L1 coefficient 
        accuracies.append(mean_acc) # Append current mean accuracy  
            
    return pd.DataFrame({'Learning Rate'  : adam_values, 
                         'L1 Coefficient' : regl_values, 
                         'Accuracy'       : accuracies})

# Plots heat map of grid search results with regularization 

def get_heat_map(heat_data, title):
    heat_data['Learning Rate'] = heat_data['Learning Rate'].round(4)
    heat_data['L1 Coefficient'] = heat_data['L1 Coefficient'].round(6) 
    heat_data_pivot = heat_data.pivot('Learning Rate', 'L1 Coefficient', 'Accuracy') 
    heat_map = sns.heatmap(heat_data_pivot).set_title('Mean Accuracies for ' + title)
    
    return heat_map

# Plots results of grid search without regularization 

def get_plot(data, title):
    plt.plot(data['Learning Rate'], data['Accuracy'])
    plt.title(title)
    plt.xlabel('Learning Rate')
    plt.ylabel('Mean Accuracy')

m1_history = grid_search([12], adam_alpha, X_train, y_train)
m1_history_regl = grid_search_regl([12], adam_alpha, regl_coeff, X_train, y_train)

get_plot(m1_history, 'Model 1 : [12]')

fig1 = get_heat_map(m1_history_regl, 'Model 1 : [12]')

m2_history = grid_search([12, 9], adam_alpha, X_train, y_train)
m2_history_regl = grid_search_regl([12, 9], adam_alpha, regl_coeff, X_train, y_train)

get_plot(m2_history, 'Model 2 : [12, 9]')

fig2 = get_heat_map(m2_history_regl, 'Model 2 : [12, 9]')

m3_history = grid_search([12, 9, 6], adam_alpha, X_train, y_train)
m3_history_regl = grid_search_regl([12, 9, 6], adam_alpha, regl_coeff, X_train, y_train)

get_plot(m3_history, 'Model 3 : [12, 9, 6]')

fig3 = get_heat_map(m3_history_regl, 'Model 3 : [12, 9, 6]')

m4_history = grid_search([12, 9, 6, 3], adam_alpha, X_train, y_train)
m4_history_regl = grid_search_regl([12, 9, 6, 3], adam_alpha, regl_coeff, X_train, y_train)

get_plot(m4_history, 'Model 4 : [12, 9, 6, 3]')

fig4 = get_heat_map(m4_history_regl, 'Model 4 : [12, 9, 6, 3]')

# returns : accuracy on test (hold out) set using the optimal hyperparameters for each model 
#           found in grid search above 

def final_results(mspec, adam, regl, X_train, y_train, X_test, y_test):

    tf.random.set_seed(123)
    np.random.seed(123)
    random.seed(123)
    
    accuracies, predictions = [], [] # Stores accuracies and predictions of model 
    
    for i in range(len(mspec)): # For each model 
        model = Sequential() 
        opt_adam = tf.keras.optimizers.Adam(learning_rate=adam[i]) # Specified learning rate 
        
        for m in mspec[i]: # Add hidden layers to model based on model specification
            if regl == 0: # If regularization specified 
                model.add(layers.Dense(m, activation='relu'))

            else: # If regularization not specified 
                model.add(layers.Dense(m, activation='relu', 
                                       kernel_regularizer=l1(regl[i]))) # Specified L1 coefficient 
        
        model.add(layers.Dense(1, activation='sigmoid')) # Final output layer 
        
        model.compile(optimizer=opt_adam, loss=BinaryCrossentropy(),
                      metrics=['accuracy'])
        
        model.fit(X_train, y_train, epochs=30, verbose=0)
        
        loss, acc = model.evaluate(X_test, y_test, verbose=0)
        accuracies.append(acc) # Record test accuracy 
        
        pred = model.predict(X_test)
        predictions.append(pred) # Record predictions on test set (as probabilities)
        
    return accuracies, predictions

# Model specifications 
models = [[12], [12,9], [12,9,6], [12,9,6,3]]

# Indices of optimal learning rates 
idx_m1 = np.argmax(m1_history.Accuracy) 
idx_m2 = np.argmax(m2_history.Accuracy)
idx_m3 = np.argmax(m3_history.Accuracy)
idx_m4 = np.argmax(m4_history.Accuracy)

# List of optimal learning rates from model 1 to model 4
adams = [m1_history['Learning Rate'][idx_m1], 
         m2_history['Learning Rate'][idx_m2],
         m3_history['Learning Rate'][idx_m3],
         m4_history['Learning Rate'][idx_m4]]

# Print optimal learning rates from model 1 to model 4 
print(adams) 

# Get accuracies and predictions on test set from NN models with NO regularization 
a, p = final_results(models, adams, 0, X_train, y_train, X_test, y_test)

# Indices of optimal hyperparameters 

idx_m1 = np.argmax(m1_history_regl.Accuracy)
idx_m2 = np.argmax(m2_history_regl.Accuracy)
idx_m3 = np.argmax(m3_history_regl.Accuracy)
idx_m4 = np.argmax(m4_history_regl.Accuracy)

# Optimal learning rates from model 1 to model 4
adams = [m1_history_regl['Learning Rate'][idx_m1],
         m2_history_regl['Learning Rate'][idx_m2],
         m3_history_regl['Learning Rate'][idx_m3],
         m4_history_regl['Learning Rate'][idx_m4]]

# Optimal L1 coefficients from model 1 to model 4
regls = [m1_history_regl['L1 Coefficient'][idx_m1],
         m2_history_regl['L1 Coefficient'][idx_m2],
         m3_history_regl['L1 Coefficient'][idx_m3],
         m4_history_regl['L1 Coefficient'][idx_m4]]

print(adams) # Print optimal learning rates from model 1 to model 4 
print(regls) # Print optimal L1 coefficients from model 1 to model 4 

# Get accuracies and predictions on test set from NN models WITH regularization 
a_r, p_r = final_results(models, adams, regls, X_train, y_train, X_test, y_test)

# Summary of final results on test set 
summary = pd.DataFrame({'Model' : [[12], [12,9], [12,9,6], [12,9,6,3]], 
                        'Test Accuracy w/o L1 Regularization'  : a,
                        'Test Accuracy w/ L1 Regularization'   : a_r})

summary.index = summary['Model']
summary.drop('Model', axis=1)

# Model 1 : [12] w/o L1 Regularization is optimal model

# Confusion matrix of optimal NN model 
tf.math.confusion_matrix(labels=y_test, predictions=p[0], num_classes=2)

# FPR and TPR from optimal NN model 
fpr, tpr, thresholds = roc_curve(y_test, p[0])

# ROC curve from optimal NN model 
line45 = np.linspace(0, 1, 100)

plt.plot(fpr, tpr)
plt.plot(line45, line45, linestyle="--")
plt.title("ROC Curve for Optimal NN Model : [12]")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")

# AUC from optimal NN model 
auc(fpr, tpr)

# Code to obtain loss convergence from fitting the optimal NN model to the train set  

tf.random.set_seed(123)
np.random.seed(123)
random.seed(123)

optimal_model = Sequential([
    layers.Dense(12, activation='relu', input_shape=(20,)), 
    layers.Dense(1,  activation='sigmoid')                 
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

optimal_model.compile(optimizer=optimizer, loss=BinaryCrossentropy(), 
                      metrics=['accuracy'])

optimal_fit = optimal_model.fit(X_train, y_train, epochs=30, 
                                verbose=0, validation_split=0.3)

# Plot loss convergence from fitting the optimal NN model to the train set  
plot_loss(optimal_fit, 'Optimal NN Model : [12]')

# Print out of the grid search for the optimal NN model 
print(m1_history)

"""Random Forest Analysis (Daniel Wang)"""

from google.colab import drive
drive.mount('/content/drive')

import os
path = "/content/drive/My Drive/425 Project" # Your path here
os.chdir(path)

#importing file with clean ICD data
import pandas as pd
final=pd.read_csv('final.csv')
final.drop(['Unnamed: 0'], axis=1, inplace=True)

final

# importing modules
import numpy as np
import pandas as pd
from sklearn import preprocessing
import matplotlib.pyplot as plt
import seaborn as sns

# splitting data into x and y values
# df_final_X=final.iloc[:,1:21]
# df_final_Y=final['POV']
final_X=np.array(final.iloc[:,1:21])
final_Y=np.array(final['POV'])

# outputting feature list
feature_list = list(df_final_X.columns)

# splitting data into test and train samples
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(final_X,final_Y,test_size=0.30)

# verifying data was split correctly
print('Training Features Shape:', X_train.shape)
print('Training Labels Shape:', Y_train.shape)
print('Testing Features Shape:', X_test.shape)
print('Testing Labels Shape:', Y_test.shape)

# setting the grid of hyperparameters
from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]
# The function to measure the quality of a split.
criterion = ['gini', 'entropy']
# Number of features to consider at every split
max_features = ['sqrt', 'log2']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 100, num = 5)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'criterion': criterion,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}
print(random_grid)

# running randomized grid search with CV
from sklearn.ensemble import RandomForestClassifier
# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestClassifier()
# Random search of parameters, using 5 fold cross validation, 
# search across 50 different combinations
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 5, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X_train, Y_train)

# outputting best parameters from randomized grid search
 rf_random.best_params_

# setting the grid of hyperparameters
from sklearn.model_selection import RandomizedSearchCV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 5)]
# The function to measure the quality of a split.
criterion = ['gini']
# Number of features to consider at every split
max_features = ['log2']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(1, 10, num = 5)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 3]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'criterion': criterion,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}
print(random_grid)

# running randomized grid search with CV
from sklearn.ensemble import RandomForestClassifier
# Use the random grid to search for best hyperparameters
# First create the base model to tune
rf = RandomForestClassifier()
# Random search of parameters, using 5 fold cross validation, 
# search across 50 different combinations
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 50, cv = 5, verbose=2, random_state=42, n_jobs = -1)
# Fit the random search model
rf_random.fit(X_train, Y_train)

# outputting best parameters from randomized grid search
 rf_random.best_params_

from sklearn.model_selection import GridSearchCV
# Create the parameter grid based on the results of random search 
param_grid = {
    'criterion': ['gini'], 
    'max_depth': [3, 5, 7],
    'max_features': [4, 5],
    'min_samples_leaf': [2, 3],
    'min_samples_split': [4, 5, 6],
    'n_estimators': [300, 400]
}
# Create a based model
rf = RandomForestClassifier()
# use grid search model parameters
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 5, n_jobs = -1, verbose = 2)

# Fit the grid search to the data
grid_search.fit(X_train, Y_train)
grid_search.best_params_

# outputting the optimal hyperparameter values
best_grid=grid_search.best_estimator_

# reshape ground truth values for confusion matrix input
y_truth=Y_test.reshape((4803, 1))

# creating confusion matrix
from sklearn.metrics import confusion_matrix
predictions = best_grid.predict(X_test)
confusion_matrix(Y_test, predictions)

# creating classification report
from sklearn.metrics import classification_report
print(classification_report(Y_test, predictions))

# outputing model accuracy
from sklearn.metrics import accuracy_score
accuracy_score(Y_test, predictions)

train_rf_predictions = best_grid.predict(X_train)
train_rf_probs = best_grid.predict_proba(X_train)[:, 1]

rf_predictions = best_grid.predict(X_test)
rf_probs = best_grid.predict_proba(X_test)[:, 1]

from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve
def evaluate_model(predictions, probs, train_predictions, train_probs):
    """Compare machine learning model to baseline performance.
    Computes statistics and shows ROC curve."""
    
    baseline = {}
    
    baseline['recall'] = recall_score(Y_test, [1 for _ in range(len(Y_test))])
    baseline['precision'] = precision_score(Y_test, [1 for _ in range(len(Y_test))])
    baseline['roc'] = 0.5
    
    results = {}
    
    results['recall'] = recall_score(Y_test, predictions)
    results['precision'] = precision_score(Y_test, predictions)
    results['roc'] = roc_auc_score(Y_test, probs)
    
    train_results = {}
    train_results['recall'] = recall_score(Y_train, train_predictions)
    train_results['precision'] = precision_score(Y_train, train_predictions)
    train_results['roc'] = roc_auc_score(Y_train, train_probs)
    
    for metric in ['recall', 'precision', 'roc']:
        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')
    
    # Calculate false positive rates and true positive rates
    base_fpr, base_tpr, _ = roc_curve(Y_test, [1 for _ in range(len(Y_test))])
    model_fpr, model_tpr, _ = roc_curve(Y_test, probs)

    plt.figure(figsize = (8, 6))
    plt.rcParams['font.size'] = 16
        # Plot both curves
    plt.plot(base_fpr, base_tpr, 'b')
    plt.plot(model_fpr, model_tpr, 'r', label = 'RF model')
    plt.legend();
    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curve');

evaluate_model(rf_predictions, rf_probs, train_rf_predictions, train_rf_probs)

import pandas as pd
feature_imp = pd.Series(best_grid.feature_importances_,index=feature_list).sort_values(ascending=False)
feature_imp

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Creating a bar plot
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()